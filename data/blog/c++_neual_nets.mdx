---
title: Multilayer Perceptrons From Scratch in C++
output:
  pdf_document:
    extra_dependencies:
      amsmath: null
      amssymb: null
      graphicx: null
      hyperref: ['colorlinks=true', 'allcolors=blue']
      listing: null

date: '2023-05-22'
tags: ['C++', 'neural networks', 'machine learning']
draft: true
summary: 'summary here'
layout: PostSimple
---

## Introduction

## Implementing Tools From Linear Algebra

Before constructing our MLP, it's useful to first implement a Matrix class with typical
matrix operations (scalar multiplication, transpose, etc.).
While this may appear unnecessary, since we can represent a neural network's parameters as, perhaps,
a vector of vectors, parameters are canonically introduced using matrices, so representing them in a
standard way can simplify implementing fast matrix multiplication and facilitate better understanding
of the code for backpropagation. If you would rather skip this section and not worry about writing
this on your own, I suggest familiarizing yourself with Eigen ^[Check out [Eigen](https://eigen.tuxfamily.org/index.php?title=Main_Page)
as an alternative to building matrices on your own.] instead.

```cpp:matrix.cpp
class Matrix {
    public:
        size_t num_rows;
        size_t num_cols;
        size_t num_entries;
        // colums are stacked in order
        std::vector<double> entries;
        std::tuple<size_t, size_t> shape;

    Matrix(size_t num_rows, size_t num_cols):
    num_rows(num_rows), num_cols(num_cols), entries({}) {
        entries.resize(num_rows * num_cols);
        shape = {num_rows, num_cols};
    }
    Matrix() : num_rows(0), num_cols(0), entries({}) {
        shape = {0, 0};
    }
}
```

The above implementation is fairly straightforward, where we are creating a matrix to be a
vector representation of the column vector of our matrix; We represent the entries of the $i$-th column vector of our
matrix (from the top down) as the vector with entries from `entries[num_rows * i]` to `entries[num_rows * i + num_rows - 1]`
inclusive. Now, it should be clear how to write basic functions for our `Matrix` class, such as printing out it's entries.
Try this on your own, then compare it to the code below.

```cpp:matrix.cpp
void print() {
        if (num_rows == 0 && num_cols == 0) {
            std::cout << "Matrix object is empty\n";
        }
        else {
            for (size_t i = 0; i < num_cols; i++) {
                for (size_t j = 0; j < num_rows; j++) {
                    std::cout << entries[num_rows * i + j] << " ";
                }
                std::cout << std::endl;
            }
            std::cout << std::endl;
        }
    }
```

For the sake of brevity, I'm not going to walk through each method's implementation step-by-step. Rather, I'm going to provide
the code upfront, so we can focus our attention on building an MLP. Most, if not all, of these concepts which are implemented below should be familiar from
an introductory linear algebra course, the one outline being fast matrix multiplication. Of course, the naive way to implement the multiplication of two $n\times n$ matrices
is to write an $\mathcal{O}(n)^3$ algorithm. In our case, this might look something like the following.

```cpp:matrix.cpp
// naive matrix multiplication
Matrix multiply(Matrix &other_matrix) {
    // assumes num_cols == matrix.num_rows
    Matrix new_matrix(other_matrix.num_cols, num_rows);
    for (size_t i = 0; i < new_matrix.num_rows; i++) {
        for (size_t j = 0; j < new_matrix.num_cols; j++) {
            for (size_t k = 0; k < other_matrix.num_rows; k ++) {
                new_matrix(i, j) += (*this)(j, k) * other_matrix(k, i);
            }
        }
    }
    return new_matrix;
}
```

Instead, we can implement this in a faster way using **\_**. Continuing, we have our complete implementatio
of `Matrix` below.

## A short review of neural networks and backpropagation
